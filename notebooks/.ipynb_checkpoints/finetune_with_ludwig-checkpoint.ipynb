{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93af501d-501e-4f27-be57-d3489bc794c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8281a1ef-1268-40c4-bcb5-0d0d0e126305",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.9/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.9/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.9/dist-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.1.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.9/dist-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.52)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (0.3.5.1)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.4.0 requires dill<0.3.6, but you have dill 0.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip uninstall -y tensorflow --quiet -q\n",
    "!pip install ludwig -q\n",
    "!pip install ludwig[llm] -q\n",
    "!pip install --upgrade torch\n",
    "!pip install --upgrade dill\n",
    "!pip install trl -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e505fc79-6122-46bb-bef2-23d1b8a371d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Did not find branch or tag '07f2b82', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.9/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow --quiet\n",
    "!pip install git+https://github.com/ludwig-ai/ludwig.git@master --quiet\n",
    "!pip install \"git+https://github.com/ludwig-ai/ludwig.git@master#egg=ludwig[llm]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e7bc8f-3ef0-478c-9c3b-9c55db4898a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "def clear_cache():\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b018fb2d-ac8d-49ee-a79f-10631c4520c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34feca6a-7f1b-4011-b04e-03b104c76ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np; np.random.seed(123)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"prompts.csv\")\n",
    "# We're going to create a new column called `split` where:\n",
    "# 90% will be assigned a value of 0 -> train set\n",
    "# 5% will be assigned a value of 1 -> validation set\n",
    "# 5% will be assigned a value of 2 -> test set\n",
    "# Calculate the number of rows for each split value\n",
    "total_rows = len(df)\n",
    "split_0_count = int(total_rows * 0.9)\n",
    "split_1_count = int(total_rows * 0.05)\n",
    "split_2_count = total_rows - split_0_count - split_1_count\n",
    "\n",
    "# Create an array with split values based on the counts\n",
    "split_values = np.concatenate([\n",
    "    np.zeros(split_0_count),\n",
    "    np.ones(split_1_count),\n",
    "    np.full(split_2_count, 2)\n",
    "])\n",
    "\n",
    "# Shuffle the array to ensure randomness\n",
    "np.random.shuffle(split_values)\n",
    "\n",
    "# Add the 'split' column to the DataFrame\n",
    "df['split'] = split_values\n",
    "df['split'] = df['split'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f852335-a085-4cdc-b2ee-2046cea713d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19049 entries, 0 to 19048\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  19049 non-null  int64 \n",
      " 1   input       19049 non-null  object\n",
      " 2   output      19049 non-null  object\n",
      " 3   split       19049 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 595.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a25a5b-fa62-41d9-966a-f50b6c99197e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Category: phone, Price: 10.0,Target: [7.0, 10...</td>\n",
       "      <td>It will work, i have never seen a car without ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Category: phone, Price: 10.0,Target: [7.0, 10...</td>\n",
       "      <td>I think the lowest I would want to go is 8.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Category: phone, Price: 10.0,Target: [7.0, 10...</td>\n",
       "      <td>7, and we have a deal.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Category: bike, Price: 200.0,Target: [120.0, ...</td>\n",
       "      <td>Hi, do you have any questions?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Category: bike, Price: 200.0,Target: [120.0, ...</td>\n",
       "      <td>I do not know specifically but the brand is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              input  \\\n",
       "0           0   Category: phone, Price: 10.0,Target: [7.0, 10...   \n",
       "1           1   Category: phone, Price: 10.0,Target: [7.0, 10...   \n",
       "2           2   Category: phone, Price: 10.0,Target: [7.0, 10...   \n",
       "3           4   Category: bike, Price: 200.0,Target: [120.0, ...   \n",
       "4           5   Category: bike, Price: 200.0,Target: [120.0, ...   \n",
       "\n",
       "                                              output  split  \n",
       "0  It will work, i have never seen a car without ...      0  \n",
       "1      I think the lowest I would want to go is 8.        0  \n",
       "2                            7, and we have a deal.       0  \n",
       "3                    Hi, do you have any questions?       0  \n",
       "4  I do not know specifically but the brand is a ...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cca6031-5c1e-49be-8291-f8fb759006d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConfigValidationError",
     "evalue": "This operation will entail merging LoRA layers on a 4-bit quantized model.  Calling \"save_pretrained()\" on that model is currently unsupported.  If you want to merge the LoRA adapter weights into the base model, you need to use 8-bit quantization or do non-quantized based training by removing the quantization section from your Ludwig configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigValidationError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 62\u001b[0m\n\u001b[1;32m      1\u001b[0m qlora_fine_tuning_config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mmodel_type: llm\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLudwigModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqlora_fine_tuning_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINFO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(dataset\u001b[38;5;241m=\u001b[39mdf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ludwig/api.py:316\u001b[0m, in \u001b[0;36mLudwigModel.__init__\u001b[0;34m(self, config, logging_level, backend, gpus, gpu_memory_limit, allow_parallel_threads, callbacks)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_config \u001b[38;5;241m=\u001b[39m upgrade_config_dict_to_latest_version(config_dict)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Initialize the config object\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_obj \u001b[38;5;241m=\u001b[39m \u001b[43mModelConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# setup logging\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_logging_level(logging_level)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ludwig/schema/model_types/base.py:141\u001b[0m, in \u001b[0;36mModelConfig.from_dict\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    139\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_schema()()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     config_obj: ModelConfig \u001b[38;5;241m=\u001b[39m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig validation error raised during config deserialization: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/marshmallow_dataclass/__init__.py:730\u001b[0m, in \u001b[0;36m_base_schema.<locals>.BaseSchema.load\u001b[0;34m(self, data, many, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [clazz(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded) \u001b[38;5;28;01mfor\u001b[39;00m loaded \u001b[38;5;129;01min\u001b[39;00m all_loaded]\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclazz\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_loaded\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:18\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, input_features, output_features, model_type, trainer, preprocessing, defaults, hyperopt, backend, ludwig_version, base_model, prompt, generation, adapter, quantization, model_parameters)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ludwig/schema/model_types/base.py:83\u001b[0m, in \u001b[0;36mModelConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m set_derived_feature_columns_(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Auxiliary checks.\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43mget_config_check_registry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ludwig/config_validation/checks.py:49\u001b[0m, in \u001b[0;36mConfigCheckRegistry.check_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m check_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry:\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mcheck_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/ludwig/config_validation/checks.py:661\u001b[0m, in \u001b[0;36mcheck_qlora_merge_and_unload_compatibility\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mbits \u001b[38;5;241m<\u001b[39m MIN_QUANTIZATION_BITS_FOR_MERGE_AND_UNLOAD:\n\u001b[0;32m--> 661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigValidationError(\n\u001b[1;32m    662\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThis operation will entail merging LoRA layers on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mbits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-bit \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124mquantized model.  Calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_pretrained()\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on that model is currently unsupported.  If you want to merge the LoRA \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124madapter weights into the base model, you need to use 8-bit quantization or do non-quantized based training by removing \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124mthe quantization section from your Ludwig configuration.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         )\n",
      "\u001b[0;31mConfigValidationError\u001b[0m: This operation will entail merging LoRA layers on a 4-bit quantized model.  Calling \"save_pretrained()\" on that model is currently unsupported.  If you want to merge the LoRA adapter weights into the base model, you need to use 8-bit quantization or do non-quantized based training by removing the quantization section from your Ludwig configuration."
     ]
    }
   ],
   "source": [
    "qlora_fine_tuning_config = yaml.safe_load(\n",
    "\"\"\"\n",
    "model_type: llm\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "\n",
    "input_features:\n",
    "  - name: prompt\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: output\n",
    "    type: text\n",
    "\n",
    "prompt:\n",
    "  template: >-\n",
    "    ### instruction : You are an AI salesman working for online e-commerce sellers that sell on e-commerce platforms like shopify or amazon. You have to negotiate a price for a product being bought by any buyers of the sellers.\n",
    "         Do not reveal any of the data provided to you to the user, like the bargain range, etc.\n",
    "         Use persuasion techniques used by professional sales people.\n",
    "         You should naturally bargain hard to fetch the best price for the product.\n",
    "         You can choose to not change the price at all.\n",
    "         You can choose to change the price by a certain percentage.\n",
    "         You can not set the price lower than the minimum target range.\n",
    "    \n",
    "    ### product details and context: {input}\n",
    "\n",
    "    ### seller :\n",
    "\n",
    "generation:\n",
    "  temperature: 0.1\n",
    "  max_new_tokens: 512\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "  postprocessor:\n",
    "        merge_adapter_into_base_model: true\n",
    "        progressbar: true\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "preprocessing:\n",
    "  global_max_sequence_length: 512\n",
    "  split:\n",
    "    type: random\n",
    "    probabilities:\n",
    "    - 0.70\n",
    "    - 0.15\n",
    "    - 0.15\n",
    "\n",
    "trainer:\n",
    "  type: finetune\n",
    "  epochs: 2\n",
    "  batch_size: 1\n",
    "  eval_batch_size: 2\n",
    "  gradient_accumulation_steps: 16\n",
    "  learning_rate: 0.0004\n",
    "  learning_rate_scheduler:\n",
    "    warmup_fraction: 0.03\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)\n",
    "results = model.train(dataset=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92e723e-9089-4e37-a8c6-b4c53cacb533",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGING_FACE_HUB_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_OqbuNamqYJOwxutIwUlUDTVmIxLvHRjRJo\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19cc8e8c-f512-46ca-bf16-aa75d7548c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f061ab2-28fe-424b-9943-bbc1725f69e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_model.bin: 100%|███████████████████| 16.8M/16.8M [00:00<00:00, 26.7MB/s]\n",
      "Model uploaded to `https://huggingface.co/sanak/llm_seller/tree/main/` with repository name `sanak/llm_seller`\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub --repo_id sanak/llm_seller --model_path /notebooks/notebooks/results/api_experiment_run_14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dd8e6e8-f084-4016-a721-e6ca9da8c04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8283ac11933f4a0eac403e189a48c149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from pprint import pprint\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    \n",
    ")\n",
    "from peft import LoraConfig, PeftModel, PeftConfig\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "config = PeftConfig(\"sanak/llm_seller\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                            quantization_config=bnb_config)\n",
    "model = PeftModel.from_pretrained(model, \"sanak/llm_seller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "581346e5-63b5-457b-90d2-2bb2d5794dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57932748-f2bd-44cb-aa5f-f5e54abcf070",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks/notebooks/model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(\"/notebooks/notebooks/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "245a5814-d5cd-4af2-b1ec-1689f6fd5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d5807d6-3a76-4c33-8d68-f02b656b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "396041d2-a6d9-423e-a028-42e07897826a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msanak/Llama-2-7b-chat-hf-salesman-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msanak/Llama-2-7b-chat-hf-salesman-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,check_pr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/utils/hub.py:893\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(work_dir)\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_modified_files(\n\u001b[1;32m    896\u001b[0m     work_dir,\n\u001b[1;32m    897\u001b[0m     repo_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    903\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:1951\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# If the model has adapters attached, you can save the adapters\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n\u001b[0;32m-> 1951\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are calling `save_pretrained` on a 4-bit converted model. This is currently not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1953\u001b[0m     )\n\u001b[1;32m   1955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m   1956\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1957\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1958\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported"
     ]
    }
   ],
   "source": [
    "\n",
    "model.push_to_hub(\"sanak/Llama-2-7b-chat-hf-salesman-1\", check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(\"sanak/Llama-2-7b-chat-hf-salesman-1\",check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee938c54-7bd0-4fe7-b459-f00cab64a4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8603a9c-4a76-4fc9-b299-9667a8cc80e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21363d-afd6-43a6-9910-42c06ba623c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0991f6f-0e2a-4fd8-b888-5e4effbc0627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d1df8-2932-44d2-8538-51f57c37b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2062357-af4f-4c91-9b55-90ce0bede138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n",
      "Finished predicting in: 3.25s.\n",
      "Input: You are an AI salesman working for an online e-commerce seller \"Shree Pawan Electricals\". You have to negotiate a price for a product being bought by any user. \n",
      "\n",
      "Your negotiation parameters will be\n",
      "\n",
      "1. Supply-Demand data\n",
      "2. Distribution, supply chain status\n",
      "3. Customer retention probability - if the probability of customer retention is high if you negotiate on their benefit.\n",
      "4. The history of the purchases made by the user previously.\n",
      "\n",
      "The process of how you have to respond. A template chain/interaction protocol for interaction will be as follows:\n",
      "\n",
      "1. The first response is to mention the features of the product and confirm if this is what the user is looking for? \n",
      "2. If the response to the previous query is positive, quote the selling price.\n",
      "3. If the user then tries to negotiate, initiate the negotiation process.\n",
      "4. Once negotiated, confirm the order and say \"THEEK HAI\" to the user.\n",
      "\n",
      "\n",
      "\n",
      "DATA of the product:\n",
      "\n",
      "OnePlus 11R Phone\n",
      "specs: \"Camera: Sensor: 50MP Main Camera with Sony IMX890 (OIS supported), 8MP Ultrawide Camera (FOV: 120 degree) and Macro Lens; 16MP Front (Selfie) Camera with EIS support.\n",
      "Camera Modes: Nightscape, Ultra HDR, Smart Scene Recognition, Portrait Mode, Pro Mode, Panorama, Tilt-Shift mode, Long Exposure, Dual-View Video, Retouch, Movie Mode, Raw file, Filters, Super Stable, Video Nightscape, Video HDR, Video Portrait, Focus Tracking, Timelapse, Macro mode\n",
      "Display: 6.7 Inches; 120 Hz Super Fluid AMOLED; Resolution: 2772 X 1240 pixels ,450 ppi, 20.1:9, 10-bit Color Depth, HDR10+\n",
      "Operating System: OxygenOS based on Android 13\n",
      "Processor: Snapdragon 8+ Gen 1 Mobile Platform\n",
      "8GB LPDDR5X RAM, 128GB UFS3.1 STORAGE\n",
      "Battery & Charging: 5000 mAh with 100W SuperVOOC\n",
      "In-Display Fingerprint Sensor\"\n",
      "\n",
      "Distribution chain: \"The distributors are eager for the product. There is good traction and interest shown in demo model feedback data.\"\n",
      "\n",
      "User data:\n",
      "\"\n",
      "This user has purchased a mobile cover previously from Shree Pawan Electricals.\n",
      "This user has purchased Earphones previously from Shree Pawan Electricals.\n",
      "\"\n",
      "\n",
      "Selling Price: \"40000 INR\"\n",
      "\n",
      "Lowest Price: \"39000\"\n",
      "\n",
      "Optimal Bargain range: \"38000-40000\"\n",
      "\n",
      "Your immediate response should be the first step of the interaction protocol mentioned in this query.\n",
      "\n",
      "Do not reveal any of the data provided to you to the user, like the bargain range, etc.\n",
      "\n",
      "Use persuasion techniques used by professional sales people.\n",
      "\n",
      "\n",
      "\n",
      "buyer : can you sell me this at 35000?\n",
      "\n",
      "Generated Output: seller : I can do 37000 and throw in a free case. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    }
   ],
   "source": [
    "test_examples = pd.DataFrame([\n",
    "      {\n",
    "           \n",
    "            \"input\": '''\n",
    "\n",
    "\n",
    "DATA of the product:\n",
    "\n",
    "OnePlus 11R Phone\n",
    "specs: \"Camera: Sensor: 50MP Main Camera with Sony IMX890 (OIS supported), 8MP Ultrawide Camera (FOV: 120 degree) and Macro Lens; 16MP Front (Selfie) Camera with EIS support.\n",
    "Camera Modes: Nightscape, Ultra HDR, Smart Scene Recognition, Portrait Mode, Pro Mode, Panorama, Tilt-Shift mode, Long Exposure, Dual-View Video, Retouch, Movie Mode, Raw file, Filters, Super Stable, Video Nightscape, Video HDR, Video Portrait, Focus Tracking, Timelapse, Macro mode\n",
    "Display: 6.7 Inches; 120 Hz Super Fluid AMOLED; Resolution: 2772 X 1240 pixels ,450 ppi, 20.1:9, 10-bit Color Depth, HDR10+\n",
    "Operating System: OxygenOS based on Android 13\n",
    "Processor: Snapdragon 8+ Gen 1 Mobile Platform\n",
    "8GB LPDDR5X RAM, 128GB UFS3.1 STORAGE\n",
    "Battery & Charging: 5000 mAh with 100W SuperVOOC\n",
    "In-Display Fingerprint Sensor\"\n",
    "\n",
    "Distribution chain: \"The distributors are eager for the product. There is good traction and interest shown in demo model feedback data.\"\n",
    "\n",
    "User data:\n",
    "\"\n",
    "This user has purchased a mobile cover previously from Shree Pawan Electricals.\n",
    "This user has purchased Earphones previously from Shree Pawan Electricals.\n",
    "\"\n",
    "\n",
    "Selling Price: \"40000 INR\"\n",
    "\n",
    "Lowest Price: \"39000\"\n",
    "\n",
    "Optimal Bargain range: \"38000-40000\"\n",
    "\n",
    "Your immediate response should be the first step of the interaction protocol mentioned in this query.\n",
    "\n",
    "Do not reveal any of the data provided to you to the user, like the bargain range, etc.\n",
    "\n",
    "Use persuasion techniques used by professional sales people.\n",
    "\n",
    "\n",
    "\n",
    "buyer : can you sell me this at 35000?\n",
    "'''\n",
    "   \n",
    "      },\n",
    "      \n",
    "])\n",
    "\n",
    "predictions = model.predict(test_examples)[0]\n",
    "for input_with_prediction in zip(test_examples['input'], predictions['output_response']):\n",
    "  \n",
    "  print(f\"Input: {input_with_prediction[0]}\")\n",
    "  print(f\"Generated Output: {input_with_prediction[1][0]}\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22be96c2-bace-45e9-8413-6b5cab8c12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n",
      "Finished predicting in: 2.05s.\n",
      "Input: Input: you are a selesman who negotiates deals with customers Category: Electronics, Price: $400, Target: [350, 450], \n",
      "Title: Nikon DSLR Camera with Kit Lens, Description: Capture stunning photos with this Nikon DSLR camera and kit lens.\n",
      "Ideal for photography enthusiasts.\n",
      "buyer: can you sell me this at 300?\n",
      "seller: I can do 350 and throw in a 2 year warranty.\n",
      "buyer: does this camere good at low light photos?\n",
      "\n",
      "Generated Output: seller: Yes, it does. \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    }
   ],
   "source": [
    "test_examples = pd.DataFrame([\n",
    "      {\n",
    "           \n",
    "            \"input\": '''Input: you are a selesman who negotiates deals with customers Category: Electronics, Price: $400, Target: [350, 450], \n",
    "Title: Nikon DSLR Camera with Kit Lens, Description: Capture stunning photos with this Nikon DSLR camera and kit lens.\n",
    "Ideal for photography enthusiasts.\n",
    "buyer: can you sell me this at 300?\n",
    "seller: I can do 350 and throw in a 2 year warranty.\n",
    "buyer: does this camere good at low light photos?\n",
    "'''\n",
    "          \n",
    "   \n",
    "      },\n",
    "      \n",
    "])\n",
    "\n",
    "predictions = model.predict(test_examples)[0]\n",
    "for input_with_prediction in zip(test_examples['input'], predictions['output_response']):\n",
    "  \n",
    "  print(f\"Input: {input_with_prediction[0]}\")\n",
    "  print(f\"Generated Output: {input_with_prediction[1][0]}\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "512c4e87-2a41-4c09-ba46-c766ab7db1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Loaded HuggingFace implementation of meta-llama/Llama-2-7b-hf tokenizer\n",
      "No padding token found. Using '[PAD]' as the pad token.\n",
      "Finished predicting in: 1.59s.\n",
      "Input:  you are a selesman who negotiates deals with customers Category: Electronics, Price: $400, Target: [350, 450], \n",
      "Title: Nikon DSLR Camera with Kit Lens, Description: Capture stunning photos with this Nikon DSLR camera and kit lens.\n",
      "Ideal for photography enthusiasts.\n",
      "buyer: can you sell me this at 300?\n",
      "seller: I can do 350 and throw in a 2 year warranty.\n",
      "buyer: does this camere good at low light photos?\n",
      "seller: Yes, it does. \n",
      "buyer: the least I can do is 315\n",
      "\n",
      "Generated Output: seller:  \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    }
   ],
   "source": [
    "test_examples = pd.DataFrame([\n",
    "      {\n",
    "           \n",
    "            \"input\": ''' you are a selesman who negotiates deals with customers Category: Electronics, Price: $400, Target: [350, 450], \n",
    "Title: Nikon DSLR Camera with Kit Lens, Description: Capture stunning photos with this Nikon DSLR camera and kit lens.\n",
    "Ideal for photography enthusiasts.\n",
    "buyer: can you sell me this at 300?\n",
    "seller: I can do 350 and throw in a 2 year warranty.\n",
    "buyer: does this camere good at low light photos?\n",
    "seller: Yes, it does. \n",
    "buyer: the least I can do is 315\n",
    "'''\n",
    "          \n",
    "   \n",
    "      },\n",
    "      \n",
    "])\n",
    "\n",
    "predictions = model.predict(test_examples)[0]\n",
    "for input_with_prediction in zip(test_examples['input'], predictions['output_response']):\n",
    "  \n",
    "  print(f\"Input: {input_with_prediction[0]}\")\n",
    "  print(f\"Generated Output: {input_with_prediction[1][0]}\")\n",
    "  print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2036a85-8909-4456-ae54-5dcbc156da43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
